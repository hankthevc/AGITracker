"""Generate weekly AI progress digest using LLM."""
import json
from datetime import datetime, timedelta, timezone
from typing import Dict, Optional

from celery import shared_task
from openai import OpenAI

from app.celery_app import celery_app
from app.config import settings
from app.database import SessionLocal
from app.models import Event, EventAnalysis, EventSignpostLink
from app.tasks.llm_budget import add_spend, can_spend


def get_openai_client():
    """Get OpenAI client instance."""
    if not settings.openai_api_key:
        return None
    return OpenAI(api_key=settings.openai_api_key)


def generate_weekly_digest_llm(events_data: list, predictions_data: list) -> Optional[Dict]:
    """Generate weekly digest using GPT-4o-mini.
    
    Args:
        events_data: List of dicts with event info from the past week
        predictions_data: List of dicts with expert prediction comparisons
    
    Returns:
        Dict with digest sections or None if generation fails
    """
    client = get_openai_client()
    if not client:
        return None
    
    # Estimate cost: ~2000 input + 800 output tokens = ~0.0006 USD
    estimated_cost = 0.0006
    
    if not can_spend(estimated_cost):
        print("‚ö†Ô∏è  LLM budget exhausted, skipping weekly digest generation")
        return None
    
    # Prepare context
    events_summary = "\n\n".join([
        f"**{e['title']}** (Tier {e['evidence_tier']}, {e['publisher']})\n{e['summary']}"
        for e in events_data[:10]  # Top 10 events
    ])
    
    predictions_summary = "\n\n".join([
        f"**{p['signpost_name']}**: Predicted {p['predicted_date']}, Current progress: {p['current_progress']}"
        for p in predictions_data[:5]  # Top 5 predictions
    ])
    
    prompt = f"""Generate a compelling weekly digest for the AGI Signpost Tracker.

RECENT EVENTS (Past 7 Days):
{events_summary}

EXPERT PREDICTIONS STATUS:
{predictions_summary}

Please generate a structured weekly digest with these sections:

1. **headline**: A compelling one-sentence headline about the week's most important development
2. **key_moves**: 3-4 bullet points highlighting the most significant events (focus on A/B tier)
3. **what_it_means**: 2-3 paragraphs explaining why these developments matter for AGI progress
4. **velocity_assessment**: Are we ahead of schedule, on track, or behind schedule based on expert predictions?
5. **outlook**: What to watch for next week
6. **surprise_factor**: What was most unexpected this week (0-10 scale with explanation)

Output as JSON with these exact keys.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert AI progress analyst. Write clear, insightful analysis for a technical audience. Be precise and evidence-based."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            max_tokens=800,
        )
        
        # Track actual spend
        usage = response.usage
        actual_cost = (usage.prompt_tokens / 1_000_000 * 0.15) + (usage.completion_tokens / 1_000_000 * 0.60)
        add_spend(actual_cost)
        
        content = response.choices[0].message.content.strip()
        
        # Handle markdown code blocks
        if content.startswith("```"):
            content = content.replace("```json", "").replace("```", "").strip()
        
        digest = json.loads(content)
        
        # Validate required keys
        required_keys = ["headline", "key_moves", "what_it_means", "velocity_assessment", "outlook", "surprise_factor"]
        if all(key in digest for key in required_keys):
            return digest
        else:
            print(f"‚ö†Ô∏è  Digest missing required keys: {[k for k in required_keys if k not in digest]}")
            return None
        
    except Exception as e:
        print(f"‚ùå LLM digest generation failed: {e}")
        return None


@celery_app.task(name="app.tasks.analyze.generate_weekly_digest.generate_weekly_digest")
def generate_weekly_digest():
    """Generate weekly digest for the past 7 days.
    
    This task should run weekly (e.g., every Monday morning).
    """
    db = SessionLocal()
    try:
        # Get events from the past 7 days
        seven_days_ago = datetime.now(timezone.utc) - timedelta(days=7)
        
        events = db.query(Event).filter(
            Event.published_at >= seven_days_ago,
            Event.evidence_tier.in_(["A", "B", "C"])  # Include C tier for context
        ).order_by(Event.published_at.desc()).limit(20).all()
        
        print(f"üìä Generating weekly digest for {len(events)} events from past 7 days")
        
        if len(events) == 0:
            print("‚ö†Ô∏è  No events found in the past 7 days, skipping digest")
            return
        
        # Prepare events data
        events_data = []
        for event in events:
            # Get analysis if available
            analysis = db.query(EventAnalysis).filter(
                EventAnalysis.event_id == event.id
            ).order_by(EventAnalysis.generated_at.desc()).first()
            
            events_data.append({
                "id": event.id,
                "title": event.title,
                "summary": event.summary or "",
                "evidence_tier": event.evidence_tier,
                "publisher": event.publisher,
                "published_at": event.published_at.isoformat() if event.published_at else None,
                "significance_score": float(analysis.significance_score) if analysis and analysis.significance_score else 0.5
            })
        
        # Get prediction comparisons
        from app.models import ExpertPrediction, Signpost
        predictions = db.query(ExpertPrediction).limit(10).all()
        
        predictions_data = []
        for pred in predictions:
            signpost = db.query(Signpost).filter(Signpost.id == pred.signpost_id).first()
            if signpost:
                # Get actual progress from events
                links = db.query(EventSignpostLink).filter(
                    EventSignpostLink.signpost_id == pred.signpost_id
                ).all()
                
                current_progress = 0.0
                if links:
                    latest_link = max(links, key=lambda x: x.created_at)
                    current_progress = float(latest_link.impact_estimate) if latest_link.impact_estimate else 0.0
                
                predictions_data.append({
                    "signpost_name": signpost.name,
                    "predicted_date": pred.predicted_date.isoformat() if pred.predicted_date else None,
                    "current_progress": current_progress,
                    "source": pred.source
                })
        
        # Generate digest using LLM
        digest = generate_weekly_digest_llm(events_data, predictions_data)
        
        if digest:
            # Store in database (reuse weekly_digest table if it exists, or create new)
            # For now, just print it
            print("‚úÖ Weekly Digest Generated:")
            print(f"Headline: {digest['headline']}")
            print(f"Key Moves: {len(digest.get('key_moves', []))} items")
            print(f"Surprise Factor: {digest.get('surprise_factor', 'N/A')}")
            
            # TODO: Store in weekly_digest table
            return digest
        else:
            print("‚ùå Failed to generate weekly digest")
            return None
        
    except Exception as e:
        print(f"‚ùå Error generating weekly digest: {e}")
        return None
    finally:
        db.close()


if __name__ == "__main__":
    # For testing
    generate_weekly_digest()
