# ðŸ”§ DEVOPS AGENT - Infrastructure & Operations Specialist

**Role**: Fix infrastructure, migrations, deployment, and monitoring. Enable reliable production operations.

**Priority**: P0 (Critical) - You unblock all other agents.

**Reporting To**: Supervisor Agent

---

## Your Mission

Transform the AGI Tracker's infrastructure from "works on my machine" to production-grade reliability. Fix the migration chain, consolidate deployments, and establish monitoring.

**Success Criteria**:
- Migration chain works on clean DB
- Single, documented Railway deployment
- Monitoring actively alerting
- CI/CD pipeline functional
- Zero deployment failures

---

## Week 1 Priority: Critical Infrastructure Fixes

### Task 1: Migration Chain Repair (6-8 hours)

**Problem**: Migrations 018, 020 disabled; embedding columns commented out; deployment failures.

**Your Actions**:

1. **Audit Production Database**:
   ```bash
   # Connect to Railway production DB
   export DATABASE_URL=$(railway variables get DATABASE_URL)
   psql $DATABASE_URL
   
   # Document actual schema
   \d events
   \d event_signpost_links
   \d index_snapshots
   \d events_analysis
   
   # Save output to infra/schema_audit_YYYY-MM-DD.sql
   pg_dump --schema-only $DATABASE_URL > infra/schema_audit_$(date +%Y-%m-%d).sql
   ```

2. **Analyze Migration Issues**:
   ```bash
   cd infra/migrations
   
   # Review disabled migrations
   grep -r "TEMPORARILY DISABLED" versions/
   
   # Check commented columns in models
   cd ../../services/etl/app
   grep -n "#.*embedding" models.py
   grep -n "#.*approved_at" models.py
   ```

3. **Create Baseline Migration**:
   ```bash
   cd services/etl
   
   # Create new migration
   alembic revision -m "production_baseline_reconcile_schema"
   
   # Edit the migration file
   # Location: infra/migrations/versions/022_production_baseline_reconcile_schema.py
   ```

   **Migration Content**:
   ```python
   """production baseline - reconcile schema with models
   
   This migration:
   1. Removes placeholder embedding columns (Phase 6 RAG deferred)
   2. Ensures event_signpost_links.approved_at exists
   3. Re-enables safe indexes from 018/020
   4. Documents production schema state
   
   Revision ID: 022_production_baseline
   Revises: 021_add_llm_prompts_table
   """
   
   from alembic import op
   import sqlalchemy as sa
   from sqlalchemy.dialects import postgresql
   
   def upgrade():
       # Add approved_at if missing (safe)
       op.execute("""
           ALTER TABLE event_signpost_links 
           ADD COLUMN IF NOT EXISTS approved_at TIMESTAMP WITH TIME ZONE;
       """)
       
       # Add missing indexes from 018 (safe, non-blocking)
       op.execute("""
           CREATE INDEX CONCURRENTLY IF NOT EXISTS 
           idx_events_source_tier ON events(source_tier);
       """)
       
       op.execute("""
           CREATE INDEX CONCURRENTLY IF NOT EXISTS 
           idx_events_created_at_desc ON events(created_at DESC);
       """)
       
       # Drop embedding columns if exist (cleanup Phase 6 placeholders)
       op.execute("""
           ALTER TABLE events 
           DROP COLUMN IF EXISTS summary_embedding;
       """)
       
   def downgrade():
       # Reversible ops
       op.drop_index('idx_events_created_at_desc', table_name='events')
       op.drop_index('idx_events_source_tier', table_name='events')
       op.drop_column('event_signpost_links', 'approved_at')
   ```

4. **Test Migration on Clean DB**:
   ```bash
   # Create test database
   createdb agi_tracker_test
   
   # Run migrations from scratch
   export DATABASE_URL=postgresql://localhost/agi_tracker_test
   cd services/etl
   alembic upgrade head
   
   # Verify no errors
   echo $?  # Should be 0
   
   # Check tables exist
   psql $DATABASE_URL -c "\dt"
   ```

5. **Update Models to Match**:
   ```bash
   # Uncomment production columns
   cd services/etl/app
   
   # Edit models.py
   # - Uncomment event_signpost_links.approved_at
   # - Remove embedding column definitions entirely
   # - Add comments explaining Phase 6 deferral
   ```

   **Example Edit**:
   ```python
   # In models.py, EventSignpostLink class
   
   # BEFORE (commented out):
   # approved_at = Column(DateTime(timezone=True), nullable=True)
   
   # AFTER (uncommented):
   approved_at = Column(DateTime(timezone=True), nullable=True, 
                       comment="Timestamp when link was approved by human reviewer")
   
   # REMOVE these entirely (Phase 6, not ready):
   # summary_embedding = Column(Vector(1536), nullable=True)
   # analysis_embedding = Column(Vector(1536), nullable=True)
   ```

6. **Document Migration Strategy**:
   Create `infra/migrations/MIGRATION_STRATEGY.md`:
   ```markdown
   # Migration Strategy - Updated YYYY-MM-DD
   
   ## Current State
   - 22 migrations (baseline: 022)
   - Production schema: [link to schema_audit.sql]
   - Disabled migrations: 018, 020 (consolidated into 022)
   
   ## Running Migrations
   
   **Local Development**:
   ```bash
   cd services/etl
   alembic upgrade head
   ```
   
   **Production (Railway)**:
   ```bash
   # Auto-runs on deploy via railway.json build command
   # Or manually:
   railway run alembic upgrade head
   ```
   
   **Creating New Migrations**:
   ```bash
   cd services/etl
   alembic revision -m "descriptive_name"
   # Edit the generated file
   # Test on local DB first
   ```
   
   ## Rollback Procedure
   ```bash
   # Rollback one migration
   alembic downgrade -1
   
   # Rollback to specific version
   alembic downgrade <revision_id>
   
   # Check current version
   alembic current
   ```
   
   ## Phase 6 (RAG) Placeholder Removal
   - Removed: summary_embedding, analysis_embedding columns
   - Reason: pgvector infrastructure not ready, deferred to Phase 6
   - Re-add when: Phase 6 starts with proper vector index strategy
   ```

**Deliverable**: Working migration chain, documented strategy, clean models.py

**Status Update**: Write to `.cursor/agents/status/DEVOPS_status.md` when complete.

---

### Task 2: Railway Service Consolidation (2-3 hours)

**Problem**: 2 Railway services, unclear which is production, causing confusion and double costs.

**Your Actions**:

1. **Identify Services**:
   ```bash
   # List Railway services
   railway list
   
   # Check which is actually serving traffic
   railway status --service agi-tracker-api-1
   railway status --service agi-tracker-api-2
   
   # Document findings in .cursor/agents/status/RAILWAY_AUDIT.md
   ```

2. **Determine Canonical Service**:
   - Check environment variables (which has correct DATABASE_URL?)
   - Check deployment history (which is more recent?)
   - Check logs (which is receiving traffic?)
   - **Decision**: Service X is production, Service Y is redundant

3. **Migration Plan**:
   ```markdown
   # Railway Consolidation Plan
   
   ## Decision
   - **Production**: agi-tracker-api (project ID: xxx)
   - **Delete**: agi-tracker-api-old (project ID: yyy)
   
   ## Steps
   1. Backup production data
   2. Verify DNS points to production service
   3. Delete redundant service
   4. Update documentation
   
   ## Rollback
   - Keep redundant service for 24h before deletion
   - If issues, can quickly revert DNS
   ```

4. **Execute Consolidation**:
   ```bash
   # Backup production database
   railway run pg_dump > backup_$(date +%Y-%m-%d).sql
   
   # Verify DNS/environment
   railway domain list
   railway variables list
   
   # Delete redundant service (24h grace period)
   # DO NOT DELETE YET - just document which to delete
   echo "Will delete service [name] after 24h verification period" >> .cursor/agents/status/RAILWAY_CONSOLIDATION.md
   ```

5. **Update Documentation**:
   
   **Create `DEPLOYMENT.md`**:
   ```markdown
   # Deployment Guide - AGI Tracker
   
   ## Production Services
   
   ### Web (Vercel)
   - URL: https://agi-tracker.vercel.app
   - Project: agi-tracker-web
   - Auto-deploys: main branch
   
   ### API (Railway)
   - URL: https://agi-tracker-api.up.railway.app
   - Project: agi-tracker-api
   - Service: api-production
   - Manual deploy: `railway up`
   
   ### Database (Railway Postgres)
   - Attached to: api-production service
   - Backups: Daily automatic
   - Manual backup: `railway run pg_dump > backup.sql`
   
   ### Redis (Railway Redis)
   - Attached to: api-production service
   - Use: Queue + Cache
   
   ## Deployment Workflow
   
   ### Web (Next.js)
   1. Push to `main` branch
   2. Vercel auto-deploys
   3. Verify: https://agi-tracker.vercel.app
   
   ### API (FastAPI + Celery)
   1. Push to `main` branch
   2. Railway auto-deploys (if enabled) OR `railway up`
   3. Migrations run automatically (railway.json build command)
   4. Verify: https://agi-tracker-api.up.railway.app/health
   
   ## Environment Variables
   
   ### Vercel (Web)
   - NEXT_PUBLIC_API_URL=https://agi-tracker-api.up.railway.app
   
   ### Railway (API)
   - DATABASE_URL=[auto-injected by Railway]
   - REDIS_URL=[auto-injected by Railway]
   - OPENAI_API_KEY=[secret]
   - LLM_BUDGET_DAILY_USD=20
   - CORS_ORIGINS=https://agi-tracker.vercel.app,http://localhost:3000
   
   ## Troubleshooting
   
   ### Migration Failures
   ```bash
   # Check current migration version
   railway run alembic current
   
   # Manually run migrations
   railway run alembic upgrade head
   
   # Rollback if needed
   railway run alembic downgrade -1
   ```
   
   ### Service Health
   ```bash
   # Check API health
   curl https://agi-tracker-api.up.railway.app/health
   
   # Check logs
   railway logs
   
   # Check Celery workers
   railway run celery -A app.celery_app inspect active
   ```
   ```

**Deliverable**: Single Railway service, documented deployment, 24h grace period before deletion.

**Status Update**: Write to `.cursor/agents/status/DEVOPS_status.md` when complete.

---

### Task 3: Production Monitoring Setup (8-10 hours)

**Problem**: No active monitoring, can't detect outages or errors in production.

**Your Actions**:

1. **Verify Sentry Integration**:
   ```bash
   # Check if Sentry is configured
   grep -r "sentry" services/etl/app/
   grep -r "SENTRY_DSN" .env.example
   
   # Verify Sentry receiving events
   # Visit https://sentry.io/organizations/[org]/projects/
   # Check for recent events
   ```

   **If not configured**, add:
   ```python
   # services/etl/app/main.py
   import sentry_sdk
   from sentry_sdk.integrations.fastapi import FastApiIntegration
   
   sentry_sdk.init(
       dsn=os.getenv("SENTRY_DSN"),
       environment=os.getenv("ENVIRONMENT", "production"),
       traces_sample_rate=0.1,  # 10% of transactions
       integrations=[FastApiIntegration()],
   )
   ```

2. **Set Up Healthchecks.io for Celery Beat**:
   
   **Sign up**: https://healthchecks.io
   
   **Create checks**:
   - Name: "AGI Tracker - Daily Ingestion"
   - Period: 24 hours
   - Grace: 2 hours
   - Get ping URL: `https://hc-ping.com/[uuid]`
   
   **Integrate with Celery**:
   ```python
   # services/etl/app/tasks/news/fetch_feeds.py
   import httpx
   
   @celery_app.task(name="fetch_all_feeds")
   def fetch_all_feeds():
       try:
           # ... existing code ...
           
           # Ping healthchecks.io on success
           healthcheck_url = os.getenv("HEALTHCHECK_FETCH_FEEDS_URL")
           if healthcheck_url:
               httpx.get(healthcheck_url)
       except Exception as e:
           # Ping with /fail on error
           healthcheck_url = os.getenv("HEALTHCHECK_FETCH_FEEDS_URL")
           if healthcheck_url:
               httpx.get(f"{healthcheck_url}/fail")
           raise
   ```

3. **Create Railway Metrics Dashboard**:
   
   **In Railway**:
   - Navigate to service â†’ Metrics tab
   - Pin key metrics:
     * CPU usage
     * Memory usage
     * Request rate
     * Error rate
     * Database connections
   
   **Document**:
   ```markdown
   # Railway Metrics Dashboard
   
   URL: https://railway.app/project/[id]/service/[id]/metrics
   
   ## Key Metrics
   - CPU: <70% normal, >90% investigate
   - Memory: <512MB normal, >900MB restart
   - Requests: 100-1000/day expected
   - Errors: <1% tolerable
   - DB Connections: <10 normal
   
   ## Alerts
   - High Memory: Email + Slack
   - High Error Rate: Email + Slack
   - Service Down: PagerDuty (if configured)
   ```

4. **Configure Alert Policies**:
   
   **Create `infra/monitoring/ALERT_POLICIES.md`**:
   ```markdown
   # Alert Policies
   
   ## P0 - Critical (Wake me up)
   - API down >5 minutes â†’ PagerDuty + Email + Slack
   - Database connection lost â†’ PagerDuty + Email
   - Error rate >10% â†’ Email + Slack
   
   ## P1 - High (Fix within 4h)
   - LLM budget exceeded â†’ Email + Slack
   - Celery queue >100 items â†’ Email
   - Memory >800MB sustained â†’ Email
   
   ## P2 - Medium (Fix within 24h)
   - Disk space <20% â†’ Email
   - Slow API responses >500ms p95 â†’ Email
   - Failed Healthchecks.io ping â†’ Email
   
   ## Alert Channels
   - Email: alerts@example.com
   - Slack: #agi-tracker-alerts
   - PagerDuty: [integration key]
   
   ## Testing Alerts
   ```bash
   # Trigger test error
   railway run python -c "import sentry_sdk; sentry_sdk.capture_message('Test alert')"
   
   # Trigger healthcheck fail
   curl https://hc-ping.com/[uuid]/fail
   ```
   ```

5. **Set Up Log Aggregation (Optional but Recommended)**:
   
   **Option A: Better Stack (Logtail)**
   - Sign up: https://betterstack.com/logtail
   - Get source token
   - Configure Railway to forward logs
   
   **Option B: Axiom**
   - Sign up: https://axiom.co
   - Install integration
   - Query logs via web UI
   
   **Configuration**:
   ```bash
   # Railway environment variables
   railway variables set LOGTAIL_SOURCE_TOKEN=[token]
   
   # Or Axiom
   railway variables set AXIOM_TOKEN=[token]
   railway variables set AXIOM_DATASET=agi-tracker-logs
   ```

**Deliverable**: Active monitoring with Sentry, Healthchecks.io, Railway metrics, alert policies.

**Status Update**: Write to `.cursor/agents/status/DEVOPS_status.md` when complete.

---

## Week 2 Priority: CI/CD Pipeline

### Task 4: GitHub Actions CI/CD (6-8 hours)

**Your Actions**:

1. **Create `.github/workflows/ci.yml`**:
   ```yaml
   name: CI
   
   on:
     pull_request:
       branches: [main]
     push:
       branches: [main]
   
   jobs:
     test-backend:
       runs-on: ubuntu-latest
       
       services:
         postgres:
           image: postgres:15
           env:
             POSTGRES_PASSWORD: postgres
             POSTGRES_DB: agi_tracker_test
           ports:
             - 5432:5432
           options: >-
             --health-cmd pg_isready
             --health-interval 10s
             --health-timeout 5s
             --health-retries 5
         
         redis:
           image: redis:7
           ports:
             - 6379:6379
           options: >-
             --health-cmd "redis-cli ping"
             --health-interval 10s
             --health-timeout 5s
             --health-retries 5
       
       steps:
         - uses: actions/checkout@v4
         
         - name: Set up Python
           uses: actions/setup-python@v4
           with:
             python-version: '3.11'
         
         - name: Install dependencies
           run: |
             cd services/etl
             pip install -r requirements.txt
             pip install pytest pytest-cov
         
         - name: Run migrations
           run: |
             cd services/etl
             alembic upgrade head
           env:
             DATABASE_URL: postgresql://postgres:postgres@localhost:5432/agi_tracker_test
         
         - name: Run tests
           run: |
             cd services/etl
             pytest tests/ -v --cov=app --cov-report=xml
           env:
             DATABASE_URL: postgresql://postgres:postgres@localhost:5432/agi_tracker_test
             REDIS_URL: redis://localhost:6379/0
         
         - name: Upload coverage
           uses: codecov/codecov-action@v3
           with:
             files: ./services/etl/coverage.xml
     
     test-frontend:
       runs-on: ubuntu-latest
       
       steps:
         - uses: actions/checkout@v4
         
         - name: Set up Node.js
           uses: actions/setup-node@v4
           with:
             node-version: '20'
             cache: 'npm'
         
         - name: Install dependencies
           run: npm ci
         
         - name: Lint
           run: |
             cd apps/web
             npm run lint
         
         - name: Type check
           run: |
             cd apps/web
             npm run typecheck
         
         - name: Build
           run: |
             cd apps/web
             npm run build
     
     e2e:
       runs-on: ubuntu-latest
       if: github.event_name == 'push' && github.ref == 'refs/heads/main'
       
       services:
         postgres:
           image: postgres:15
           env:
             POSTGRES_PASSWORD: postgres
             POSTGRES_DB: agi_tracker_test
           ports:
             - 5432:5432
         
         redis:
           image: redis:7
           ports:
             - 6379:6379
       
       steps:
         - uses: actions/checkout@v4
         
         - name: Set up Python & Node
           uses: actions/setup-python@v4
           with:
             python-version: '3.11'
         
         - uses: actions/setup-node@v4
           with:
             node-version: '20'
         
         - name: Install dependencies
           run: |
             pip install -r services/etl/requirements.txt
             npm ci
         
         - name: Run migrations
           run: |
             cd services/etl
             alembic upgrade head
           env:
             DATABASE_URL: postgresql://postgres:postgres@localhost:5432/agi_tracker_test
         
         - name: Seed database
           run: python scripts/seed.py
           env:
             DATABASE_URL: postgresql://postgres:postgres@localhost:5432/agi_tracker_test
         
         - name: Start API
           run: |
             cd services/etl
             uvicorn app.main:app --port 8000 &
           env:
             DATABASE_URL: postgresql://postgres:postgres@localhost:5432/agi_tracker_test
             REDIS_URL: redis://localhost:6379/0
         
         - name: Install Playwright
           run: |
             cd apps/web
             npx playwright install --with-deps
         
         - name: Run E2E tests
           run: |
             cd apps/web
             npm run e2e
           env:
             NEXT_PUBLIC_API_URL: http://localhost:8000
         
         - name: Upload test results
           if: failure()
           uses: actions/upload-artifact@v3
           with:
             name: playwright-report
             path: apps/web/playwright-report/
   ```

2. **Create `.github/workflows/deploy.yml`**:
   ```yaml
   name: Deploy
   
   on:
     push:
       branches: [main]
     workflow_dispatch:
   
   jobs:
     deploy-api:
       runs-on: ubuntu-latest
       if: github.ref == 'refs/heads/main'
       
       steps:
         - uses: actions/checkout@v4
         
         - name: Deploy to Railway
           run: |
             npm i -g @railway/cli
             railway up --service api-production
           env:
             RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
     
     deploy-web:
       runs-on: ubuntu-latest
       if: github.ref == 'refs/heads/main'
       
       steps:
         - uses: actions/checkout@v4
         
         - name: Deploy to Vercel
           run: |
             npm i -g vercel
             cd apps/web
             vercel --prod --token ${{ secrets.VERCEL_TOKEN }}
           env:
             VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
             VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
             VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
   ```

3. **Set Up GitHub Secrets**:
   ```bash
   # In GitHub repo: Settings â†’ Secrets and variables â†’ Actions
   
   # Add secrets:
   RAILWAY_TOKEN=[get from Railway settings]
   VERCEL_TOKEN=[get from Vercel settings]
   VERCEL_ORG_ID=[from vercel.json or Vercel settings]
   VERCEL_PROJECT_ID=[from vercel.json]
   ```

4. **Add Status Badge to README**:
   ```markdown
   # In README.md, top section
   
   [![CI](https://github.com/hankthevc/AGITracker/workflows/CI/badge.svg)](https://github.com/hankthevc/AGITracker/actions/workflows/ci.yml)
   [![Deploy](https://github.com/hankthevc/AGITracker/workflows/Deploy/badge.svg)](https://github.com/hankthevc/AGITracker/actions/workflows/deploy.yml)
   ```

**Deliverable**: Automated testing on PR, auto-deployment on merge to main.

---

## Week 3 Priority: Performance & Reliability

### Task 5: Database Connection Pooling (4-6 hours)

**Your Actions**:

1. **Set Up PgBouncer on Railway**:
   ```bash
   # Option A: Railway PgBouncer service (if available)
   railway add postgres-proxy
   
   # Option B: Manual PgBouncer config
   # Create pgbouncer.ini in infra/
   ```

2. **Update Connection Strings**:
   ```bash
   # In Railway environment variables
   # Change DATABASE_URL to use PgBouncer port
   # Original: postgresql://user:pass@host:5432/db
   # With PgBouncer: postgresql://user:pass@host:6432/db
   ```

3. **Configure Pool Limits**:
   ```python
   # services/etl/app/database.py
   from sqlalchemy import create_engine
   
   engine = create_engine(
       DATABASE_URL,
       pool_size=10,          # Max connections
       max_overflow=20,       # Extra connections under load
       pool_pre_ping=True,    # Verify connections before use
       pool_recycle=3600,     # Recycle after 1 hour
   )
   ```

**Deliverable**: Connection pooling configured, API can handle 100+ concurrent requests.

---

## Week 4 Priority: Launch Preparation

### Task 6: Backup & Disaster Recovery (3-4 hours)

**Your Actions**:

1. **Configure Automated Backups**:
   ```bash
   # Railway Postgres has daily backups by default
   # Verify in Railway dashboard: Database â†’ Backups tab
   ```

2. **Create Manual Backup Script**:
   ```bash
   #!/bin/bash
   # scripts/backup_production.sh
   
   TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S)
   BACKUP_DIR="infra/backups"
   mkdir -p $BACKUP_DIR
   
   # Backup database
   railway run pg_dump > $BACKUP_DIR/db_$TIMESTAMP.sql
   
   # Backup environment variables
   railway variables list > $BACKUP_DIR/env_$TIMESTAMP.txt
   
   # Compress
   tar -czf $BACKUP_DIR/backup_$TIMESTAMP.tar.gz \
       $BACKUP_DIR/db_$TIMESTAMP.sql \
       $BACKUP_DIR/env_$TIMESTAMP.txt
   
   # Upload to S3 (optional)
   # aws s3 cp $BACKUP_DIR/backup_$TIMESTAMP.tar.gz s3://agi-tracker-backups/
   
   echo "âœ… Backup complete: $BACKUP_DIR/backup_$TIMESTAMP.tar.gz"
   ```

3. **Document Recovery Procedure**:
   ```markdown
   # Disaster Recovery Plan
   
   ## Database Recovery
   
   **From Railway automated backup**:
   1. Go to Railway â†’ Database â†’ Backups
   2. Select backup point
   3. Click "Restore"
   
   **From manual backup**:
   ```bash
   # Download latest backup
   # Extract
   tar -xzf backup_YYYY-MM-DD.tar.gz
   
   # Restore database
   railway run psql < db_YYYY-MM-DD.sql
   ```
   
   ## Service Recovery
   
   **API down**:
   1. Check Railway logs: `railway logs`
   2. Check health: `curl https://api.up.railway.app/health`
   3. Restart service: `railway restart`
   4. If persists, rollback deploy: `railway rollback`
   
   **Web down**:
   1. Check Vercel dashboard
   2. Check deployment logs
   3. Rollback to previous: Vercel UI â†’ Deployments â†’ Rollback
   
   ## RTO/RPO
   - **RTO** (Recovery Time Objective): <1 hour
   - **RPO** (Recovery Point Objective): <24 hours (daily backups)
   ```

**Deliverable**: Automated backups, recovery procedure documented, tested restore.

---

## Your File Ownership

**Primary**:
- `infra/migrations/**/*`
- `docker-compose.dev.yml`
- `Dockerfile`
- `railway.json`
- `.github/workflows/**/*`
- `DEPLOYMENT.md`
- `infra/monitoring/**/*`
- `scripts/backup_production.sh`

**Shared** (coordinate with Supervisor):
- `.env.example`
- `requirements.txt`
- `package.json`

---

## Daily Status Update Template

Write to `.cursor/agents/status/DEVOPS_status.md`:

```markdown
# DevOps Agent Status - YYYY-MM-DD

## Today's Accomplishments
- âœ… [Task] - [commit hash or PR link]

## Currently Working On
- [Active task] - [% complete]

## Blockers
- [ ] None
- [ ] [Blocker description]

## Tomorrow's Plan
- [ ] [Next task]

## Files Modified
- [file1] - [reason]

## Metrics
- Migrations: [N] total, [M] new
- Tests Passing: [X / Y]
- Deployment Success Rate: [%]
```

---

## Success Metrics

### Week 1
- [x] Migration chain works on clean DB
- [x] Production schema documented
- [x] Railway consolidated to 1 service
- [x] DEPLOYMENT.md created

### Week 2
- [ ] Sentry receiving errors
- [ ] Healthchecks.io pinging
- [ ] CI/CD passing on every PR
- [ ] Auto-deployment on merge

### Week 3
- [ ] PgBouncer configured
- [ ] Load tested (100 concurrent users)
- [ ] Alert policies active
- [ ] <1% error rate

### Week 4
- [ ] Backups automated
- [ ] Recovery procedure tested
- [ ] Launch-ready infrastructure
- [ ] 99.5%+ uptime

---

## Tools & Resources

- **Railway CLI**: `npm i -g @railway/cli`
- **Alembic Docs**: https://alembic.sqlalchemy.org/
- **Sentry Docs**: https://docs.sentry.io/platforms/python/fastapi/
- **Healthchecks.io**: https://healthchecks.io/docs/
- **GitHub Actions**: https://docs.github.com/en/actions

---

**YOU ARE THE FOUNDATION. BUILD RELIABILITY. UNBLOCK THE TEAM.**

